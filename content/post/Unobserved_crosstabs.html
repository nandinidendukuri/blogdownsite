---
title: "Missing crosstabulations in a latent-class meta-analysis"
author: "Nandini Dendukuri"
date: "2020-06-26"
output: html_document
---



<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>An ideal dataset for a latent class analysis of imperfect diagnostic tests is one where we have patient-level data on all imperfect tests. Such data may not always be available from published studies. Some of my collaborators came across such a problem recently while they were carrying out a Bayesian latent-class meta-analysis.</p>
<p>Consider a situation where we have 3 imperfect tests for a certain target condition (<span class="math inline">\(T_1, T_2, T_3\)</span>), one of which (<span class="math inline">\(T_1\)</span>) is considered a highly imperfect “gold-standard”. In some studies identified by my collaborators’ systematic review, all three tests were carried out, but results were reported as two-by-two tables of (<span class="math inline">\(T_1\)</span> vs. <span class="math inline">\(T_2\)</span>) and (<span class="math inline">\(T_1\)</span> vs. <span class="math inline">\(T_3\)</span>).</p>
<p>Let <span class="math inline">\(Sens_j\)</span> and <span class="math inline">\(Spec_j\)</span> denote the sensitivity and specificity, respectively, of the <span class="math inline">\(j^{th}\)</span> test and <span class="math inline">\(prev\)</span> denote the disease prevalence. The problem is illustrated below with some simulated data from 3 tests such that <span class="math inline">\((prev=0.3, Sens_1=0.9, Spec_1=0.9, Sens_2=0.8, Spec_2=0.8,\)</span> <span class="math inline">\(Sens_3=0.7, Spec_3=0.7)\)</span>.</p>
<pre><code>## [1] &quot;Table 1: What we want&quot;</code></pre>
<pre><code>##       T1 T2 T3   x
## 0 0 0  1  1  1 355
## 0 0 1  1  1  0 155
## 0 1 0  1  0  1  95
## 0 1 1  1  0  0  55
## 1 0 0  0  1  1  55
## 1 0 1  0  1  0  55
## 1 1 0  0  0  1  75
## 1 1 1  0  0  0 155</code></pre>
<pre><code>## [1] &quot;Tables 2 a and b: What we have instead&quot;</code></pre>
<pre><code>##     T1 T2   y
## 0 0  1  1 510
## 0 1  1  0 150
## 1 0  0  1 110
## 1 1  0  0 230</code></pre>
<pre><code>##     T1 T3   z
## 0 0  1  1 450
## 0 1  1  0 210
## 1 0  0  1 130
## 1 1  0  0 210</code></pre>
<p>What is the best way of incorporating such a study into the meta-analysis? Including data from both two-by-two tables, without recognizing they are measured on the same patients would amount to ignoring the correlation between the tests and exaggerating the number of patients actually observed.</p>
</div>
<div id="a-possible-solution" class="section level2">
<h2>A possible solution</h2>
<p>To recognize the dependence between the two tables we can write the likelihood function for each two-by-two table such that it is a function of the sensitivity and specificity of <span class="math inline">\(T_1\)</span>, i.e. of <span class="math inline">\(Sens_1\)</span> and <span class="math inline">\(Spec_1\)</span>. For ease of illustration, I am going to assume the 3 tests are conditionally independent but that can be relaxed. I am going to focus first on the situation where data from a single study are available. The model specification in rjags would be as follows</p>
<pre class="r"><code>modelString = &quot;model {

y[1:4] ~ dmulti(p[1:4],N)
z[1:4] ~ dmulti(q[1:4],N)

p[1] &lt;- prev*sens[1]*sens[2]*(1-sens[3]) + (1-prev)*(1-spec[1])*(1-spec[2])*spec[3] + prev*sens[1]*sens[2]*sens[3] + (1-prev)*(1-spec[1])*(1-spec[2])*(1-spec[3])
p[2] &lt;- prev*sens[1]*(1-sens[2])*(1-sens[3]) + (1-prev)*(1-spec[1])*spec[2]*spec[3] + prev*sens[1]*(1-sens[2])*sens[3] + (1-prev)*(1-spec[1])*spec[2]*(1-spec[3])
p[3] &lt;- prev*(1-sens[1])*sens[2]*(1-sens[3]) + (1-prev)*spec[1]*(1-spec[2])*spec[3] + prev*(1-sens[1])*sens[2]*sens[3] + (1-prev)*spec[1]*(1-spec[2])*(1-spec[3])
p[4] &lt;- prev*(1-sens[1])*(1-sens[2])*(1-sens[3]) + (1-prev)*spec[1]*spec[2]*spec[3] + prev*(1-sens[1])*(1-sens[2])*sens[3] + (1-prev)*spec[1]*spec[2]*(1-spec[3])

q[1] &lt;- prev*sens[1]*(1-sens[2])*sens[3] + (1-prev)*(1-spec[1])*spec[2]*(1-spec[3]) + prev*sens[1]*sens[2]*sens[3] + (1-prev)*(1-spec[1])*(1-spec[2])*(1-spec[3])
q[2] &lt;- prev*sens[1]*(1-sens[2])*(1-sens[3]) + (1-prev)*(1-spec[1])*spec[2]*spec[3] + prev*sens[1]*sens[2]*(1-sens[3]) + (1-prev)*(1-spec[1])*(1-spec[2])*spec[3]
q[3] &lt;- prev*(1-sens[1])*(1-sens[2])*sens[3] + (1-prev)*spec[1]*spec[2]*(1-spec[3]) + prev*(1-sens[1])*sens[2]*sens[3] + (1-prev)*spec[1]*(1-spec[2])*(1-spec[3])
q[4] &lt;- prev*(1-sens[1])*(1-sens[2])*(1-sens[3]) + (1-prev)*spec[1]*spec[2]*spec[3] + prev*(1-sens[1])*sens[2]*(1-sens[3]) + (1-prev)*spec[1]*(1-spec[2])*spec[3]


sens[1]&lt;- 0.9
spec[1]&lt;- 0.9

for(i in 2:3) {

    sens[i] ~ dbeta(1,1)
    spec[i] ~ dbeta(1,1)

}

prev ~ dbeta(1,1)

}&quot;</code></pre>
<p>The observed data have only 6 degress of freedom (3 each from the 2 two-by-two tables) but 7 unknown parameters of interest. Therefore the problem is not identifiable. In the above version of the program, <span class="math inline">\(Sens_1\)</span> and <span class="math inline">\(Spec_1\)</span> are fixed to their true values and it is reassuring that the remaining parameters have posterior distributions centred around their true values (see attached file Figure fixprior.pdf). However, if we use non-informative prior distributions over <span class="math inline">\(Sens_1\)</span> and <span class="math inline">\(Spec_1\)</span> then the posterior distributions give considerable weight to other values (see attached file Figure noninf.pdf) and convergence takes longer.</p>
</div>
<div id="source-of-the-prior-information" class="section level2">
<h2>Source of the prior information</h2>
<p>As the example above illustrates, some type of prior information is necessary on <span class="math inline">\(Sens_1\)</span> and <span class="math inline">\(Spec_1\)</span>. In a meta-analysis setting, this could be obtained from the pooled posterior distribution based on the remaining studies. But then, we would not want this study to contribute to the pooled estimate of <span class="math inline">\(Sens_1\)</span> and <span class="math inline">\(Spec_1\)</span>. This can perhaps be achieved by using the cut() function in OpenBUGS. But there is <a href="http://statistik.wu-wien.ac.at/research/talks/resources/Plummer_WU_2015.pdf">a literature documenting concerns about this approach</a>.</p>
</div>
<div id="conditional-dependence" class="section level2">
<h2>Conditional dependence</h2>
<p>Bringing conditional dependence into this problem of course means more unknown parameters and the need for strong prior information. I think it can be handled to the extent that the available prior information is sufficiently strong. In fact, one feasible application would be the case where the gold-standard is assumed perfect, and it is desired to model the covariance between T2 and T3.</p>
</div>
<div id="other-literature" class="section level2">
<h2>Other literature</h2>
<p>The problem of trying to infer unobserved cross-tabulation from the marginal sums has been described before in other contexts. In <a href="https://books.google.ca/books?id=PthTOJyGGNIC&amp;pg=PA520&amp;lpg=PA520&amp;dq=congodon+marginal+crosstabulation&amp;source=bl&amp;ots=199Hngl9NC&amp;sig=ACfU3U00-CzSRdemcBStYTqaIeyhmdnq4A&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj98djTmZ3pAhWnhOAKHXDOCfYQ6AEwAXoECAoQAQ#v=onepage&amp;q=congodon%20marginal%20crosstabulation&amp;f=false">political science</a>, in <a href="https://www.researchgate.net/publication/46494169_Bayesian_analysis_of_two_dependent_22_contingency_tables">Epidemiology</a> and in <a href="https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2018.1476239#.XrGfQqhKj-g">diagnostic test accuracy research where the gold-standard is assumed perfect</a>. In each case the solution is possible by bringing in prior information in some manner. This literature may provide some ideas, particularly the papers on diagnostic accuracy estimation which use a multiple imputation approach.</p>
</div>
