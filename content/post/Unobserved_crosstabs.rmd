---
title: "Missing crosstabulations in a latent-class meta-analysis"
author: "Nandini Dendukuri"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The problem

An ideal dataset for a latent class analysis of imperfect diagnostic tests is one where we have patient-level data on all imperfect tests. Such data may not always be available from published studies. Some of my collaborators came across such a problem recently while they were carrying out a Bayesian latent-class meta-analysis. 

Consider a situation where we have 3 imperfect tests for a certain target condition ($T_1, T_2, T_3$), one of which ($T_1$) is considered a highly imperfect "gold-standard". In some studies identified by my collaborators' systematic review, all three tests were carried out, but results were reported as two-by-two tables of ($T_1$ vs. $T_2$) and ($T_1$ vs. $T_3$). 

Let $Sens_j$ and $Spec_j$ denote the sensitivity and specificity, respectively, of the $j^{th}$ test and $prev$ denote the disease prevalence. The problem is illustrated below with some simulated data from 3 tests such that $(prev=0.3, Sens_1=0.9, Spec_1=0.9, Sens_2=0.8, Spec_2=0.8,$ $Sens_3=0.7, Spec_3=0.7)$.

```{r problemfig, echo=FALSE}

# SIMULATING DATA
sens=c(0.9,0.8,0.7)
spec=c(0.9,0.8,0.7)
prev=0.3
p=rep(0,8)
N=1000

p[1] <- prev*sens[1]*sens[2]*sens[3] + (1-prev)*(1-spec[1])*(1-spec[2])*(1-spec[3])
p[2] <- prev*sens[1]*sens[2]*(1-sens[3]) + (1-prev)*(1-spec[1])*(1-spec[2])*spec[3]
p[3] <- prev*sens[1]*(1-sens[2])*sens[3] + (1-prev)*(1-spec[1])*spec[2]*(1-spec[3])
p[4] <- prev*sens[1]*(1-sens[2])*(1-sens[3]) + (1-prev)*(1-spec[1])*spec[2]*spec[3]
p[5] <- prev*(1-sens[1])*sens[2]*sens[3] + (1-prev)*spec[1]*(1-spec[2])*(1-spec[3])
p[6] <- prev*(1-sens[1])*sens[2]*(1-sens[3]) + (1-prev)*spec[1]*(1-spec[2])*spec[3]
p[7] <- prev*(1-sens[1])*(1-sens[2])*sens[3] + (1-prev)*spec[1]*spec[2]*(1-spec[3])
p[8] <- prev*(1-sens[1])*(1-sens[2])*(1-sens[3]) + (1-prev)*spec[1]*spec[2]*spec[3]

x=round(p*N)

y=rep(0,4)
z=rep(0,4)

y[1] <- x[1] + x[2]
y[2] <- x[3] + x[4]
y[3] <- x[5] + x[6]
y[4] <- x[7] + x[8]

z[1] <- x[1] + x[3]
z[2] <- x[2] + x[4]
z[3] <- x[5] + x[7]
z[4] <- x[6] + x[8]

print("Table 1: What we want")
T1=rep(c(1,1,1,1,0,0,0,0),x)
T2=rep(c(1,1,0,0,1,1,0,0),x)
T3=rep(c(1,0,1,0,1,0,1,0),x)
table1=cbind(c(1,1,1,1,0,0,0,0),c(1,1,0,0,1,1,0,0),c(1,0,1,0,1,0,1,0),table(paste(T1,T2,T3))[1:8])
colnames(table1)=c("T1","T2","T3","x")
print(table1)
    
print("Tables 2 a and b: What we have instead")
table2=cbind(c(1,1,0,0),c(1,0,1,0),table(paste(T1,T2))[1:4])
colnames(table2)=c("T1","T2","y")
print(table2)

table3=cbind(c(1,1,0,0),c(1,0,1,0),table(paste(T1,T3))[1:4])
colnames(table3)=c("T1","T3","z")
print(table3)

```

What is the best way of incorporating such a study into the meta-analysis? Including data from both two-by-two tables, without recognizing they are measured on the same patients would amount to ignoring the correlation between the tests and exaggerating the number of patients actually observed.

## A possible solution

To recognize the dependence between the two tables we can write the likelihood function for each two-by-two table such that it is a function of the sensitivity and specificity of $T_1$, i.e. of $Sens_1$ and $Spec_1$. For ease of illustration, I am going to assume the 3 tests are conditionally independent but that can be relaxed. I am going to focus first on the situation where data from a single study are available. The model specification in rjags would be as follows

```{r rjagsmodel1, results="hide"}
modelString = "model {

y[1:4] ~ dmulti(p[1:4],N)
z[1:4] ~ dmulti(q[1:4],N)

p[1] <- prev*sens[1]*sens[2]*(1-sens[3]) + (1-prev)*(1-spec[1])*(1-spec[2])*spec[3] + prev*sens[1]*sens[2]*sens[3] + (1-prev)*(1-spec[1])*(1-spec[2])*(1-spec[3])
p[2] <- prev*sens[1]*(1-sens[2])*(1-sens[3]) + (1-prev)*(1-spec[1])*spec[2]*spec[3] + prev*sens[1]*(1-sens[2])*sens[3] + (1-prev)*(1-spec[1])*spec[2]*(1-spec[3])
p[3] <- prev*(1-sens[1])*sens[2]*(1-sens[3]) + (1-prev)*spec[1]*(1-spec[2])*spec[3] + prev*(1-sens[1])*sens[2]*sens[3] + (1-prev)*spec[1]*(1-spec[2])*(1-spec[3])
p[4] <- prev*(1-sens[1])*(1-sens[2])*(1-sens[3]) + (1-prev)*spec[1]*spec[2]*spec[3] + prev*(1-sens[1])*(1-sens[2])*sens[3] + (1-prev)*spec[1]*spec[2]*(1-spec[3])

q[1] <- prev*sens[1]*(1-sens[2])*sens[3] + (1-prev)*(1-spec[1])*spec[2]*(1-spec[3]) + prev*sens[1]*sens[2]*sens[3] + (1-prev)*(1-spec[1])*(1-spec[2])*(1-spec[3])
q[2] <- prev*sens[1]*(1-sens[2])*(1-sens[3]) + (1-prev)*(1-spec[1])*spec[2]*spec[3] + prev*sens[1]*sens[2]*(1-sens[3]) + (1-prev)*(1-spec[1])*(1-spec[2])*spec[3]
q[3] <- prev*(1-sens[1])*(1-sens[2])*sens[3] + (1-prev)*spec[1]*spec[2]*(1-spec[3]) + prev*(1-sens[1])*sens[2]*sens[3] + (1-prev)*spec[1]*(1-spec[2])*(1-spec[3])
q[4] <- prev*(1-sens[1])*(1-sens[2])*(1-sens[3]) + (1-prev)*spec[1]*spec[2]*spec[3] + prev*(1-sens[1])*sens[2]*(1-sens[3]) + (1-prev)*spec[1]*(1-spec[2])*spec[3]


sens[1]<- 0.9
spec[1]<- 0.9

for(i in 2:3) {

	sens[i] ~ dbeta(1,1)
	spec[i] ~ dbeta(1,1)

}

prev ~ dbeta(1,1)

}"
```

The observed data have only 6 degress of freedom (3 each from the 2 two-by-two tables) but 7 unknown parameters of interest. Therefore the problem is not identifiable. In the above version of the program, $Sens_1$ and $Spec_1$ are fixed to their true values and it is reassuring that the remaining parameters have posterior distributions centred around their true values (see attached file Figure fixprior.pdf). However, if we use non-informative prior distributions over $Sens_1$ and $Spec_1$ then the posterior distributions give considerable weight to other values (see attached file Figure noninf.pdf) and convergence takes longer.   

```{r jagsprg, include=FALSE}
library(rjags)
library(mcmcplots)

writeLines(modelString,con="model.txt")

dataList = list(
  y=y,
  z=z,
  N=N
)


#Initial values
initsList = list(list(sens = c(NA,0.8,0.7), spec=c(NA,0.8,0.7),prev=0.3),
                 list(sens = c(NA,0.8,0.7), spec=c(NA,0.8,0.7),prev=0.4),
                 list(sens = c(NA,0.8,0.7), spec=c(NA,0.8,0.7),prev=0.2))

#Compiling
jagsModel = jags.model("model.txt",data=dataList,inits=initsList,n.chains=3)

#Burn-in
update( jagsModel,n.iter=2000)

#Sampling from the posterior distribution:
# The parameter(s) to be monitored.
parameters = c( "sens" , "spec", "prev")  

codaSamples = coda.samples(jagsModel,variable.names=parameters,n.iter=5000)
output <- codaSamples#[[2]]                         

# Assessing convergence.  Plots displayed in html format
mcmcplot(output, parms=c("sens[2]", "spec[2]","sens[3]", "spec[3]","prev"))
```

## Source of the prior information 

As the example above illustrates, some type of prior information is necessary on $Sens_1$ and $Spec_1$. In a meta-analysis setting, this could be obtained from the pooled posterior distribution based on the remaining studies. But then, we would not want this study to contribute to the pooled estimate of $Sens_1$ and $Spec_1$. This can perhaps be achieved by using the cut() function in OpenBUGS. But there is [a literature documenting concerns about this approach](http://statistik.wu-wien.ac.at/research/talks/resources/Plummer_WU_2015.pdf).  

## Conditional dependence

Bringing conditional dependence into this problem of course means more unknown parameters and the need for strong prior information. I think it can be handled to the extent that the available prior information is sufficiently strong. In fact, one feasible application would be the case where the gold-standard is assumed perfect, and it is desired to model the covariance between T2 and T3.

## Other literature

The problem of trying to infer unobserved cross-tabulation from the marginal sums has been described before in other contexts. In [political science](https://books.google.ca/books?id=PthTOJyGGNIC&pg=PA520&lpg=PA520&dq=congodon+marginal+crosstabulation&source=bl&ots=199Hngl9NC&sig=ACfU3U00-CzSRdemcBStYTqaIeyhmdnq4A&hl=en&sa=X&ved=2ahUKEwj98djTmZ3pAhWnhOAKHXDOCfYQ6AEwAXoECAoQAQ#v=onepage&q=congodon%20marginal%20crosstabulation&f=false), in [Epidemiology](https://www.researchgate.net/publication/46494169_Bayesian_analysis_of_two_dependent_22_contingency_tables) and in [diagnostic test accuracy research where the gold-standard is assumed perfect](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2018.1476239#.XrGfQqhKj-g). In each case the solution is possible by bringing in prior information in some manner. This literature may provide some ideas, particularly the papers on diagnostic accuracy estimation which use a multiple imputation approach.
