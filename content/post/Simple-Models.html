---
title: "Learning rjags using simple statistical models"
author: Nandini Dendukuri
date: '2020-06-28'
slug: learnrjags
output: 
  html_document:
#    code_folding: hide
    theme: journal
    toc: yes
    toc_float: yes
    number_sections: false
categories:
  - Blogging
tags:
  - R Markdown
  - blogdown
  - rjags
  - mcmcplot
  - Bayesian analysis
---



<p>Here are some examples of fitting simple statistical models in rjags. They are useful for beginners learning Bayesian inference using the <a href="https://cran.r-project.org/web/packages/rjags/index.html">rjags</a> library. Notice in each case all that is required is specification of the probability density function for the observed data (i.e.Â the likelihood function) and the prior distribution functions for the unknown parameters.</p>
<p>Beyond that the user needs to be familiar with the rjags syntax for compiling the model, running the MCMC algorithm, sampling from the posterior distribution and examining the sample. The <a href="https://cran.r-project.org/web/packages/mcmcplots/index.html">mcmcplots</a> library is necessary for the last step.</p>
<p>These example programs were first written in WinBUGS by <a href="http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-675/CourseOutline.html">Lawrence Joseph</a>. They were reprogrammed using R Markdown by <a href="https://nandinidendukuri.com/team/mandy-yao/">Mandy Yao</a>.</p>
<div id="loading-the-necessary-libraries" class="section level2">
<h2>Loading the necessary libraries</h2>
<pre class="r"><code>library(rjags)
library(mcmcplots)</code></pre>
</div>
<div id="binomial-proportion" class="section level2">
<h2>Binomial Proportion</h2>
<pre class="r"><code>modelString = 
  &quot;model { 
  
    x~dbin(theta,n)        #  Likelihood function

    theta ~ dbeta(1,1)     #  Prior density for theta
    
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=6,n=20))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1
##    Unobserved stochastic nodes: 1
##    Total graph size: 4
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update(jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;theta&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# Density plots
denplot(output, parms=c(&quot;theta&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># History plot(s)
traplot(output, parms=c(&quot;theta&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##      0.3195139      0.0968389      0.0009684      0.0013001 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## 0.1489 0.2503 0.3133 0.3837 0.5218</code></pre>
</div>
<div id="binomial-proportion-difference" class="section level2">
<h2>Binomial Proportion Difference</h2>
<pre class="r"><code>modelString = 
  &quot;model { x  ~ dbin(theta1, n1)   #  Likelihood for group 1
         theta1 ~ dbeta(1,1)         #  Prior for theta1

         y  ~ dbin(theta2, n2)   #  Likelihood for group 2
         theta2 ~ dbeta(1,1)         #  Prior for theta2

      propdiff &lt;- theta1-theta2        #  Calculate difference for binomial parameters
      rr &lt;- theta1/theta2              #  Calculate relative risk
                                       #  Calculate odds ratio
      or&lt;- theta1*(1-theta2)/((1-theta1)*theta2)
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x  = 6,
                                             n1 = 20,
                                             y  = 20,
                                             n2 = 25))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 2
##    Unobserved stochastic nodes: 2
##    Total graph size: 14
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;theta1&quot;, &quot;theta2&quot;, &quot;propdiff&quot;, &quot;or&quot;, &quot;rr&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# History plots
traplot(output, parms=c(&quot;propdiff&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;propdiff&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean      SD  Naive SE Time-series SE
## or        0.1521 0.10839 0.0010839       0.001474
## propdiff -0.4569 0.12557 0.0012557       0.001672
## rr        0.4162 0.13631 0.0013631       0.001808
## theta1    0.3197 0.09818 0.0009818       0.001296
## theta2    0.7766 0.07883 0.0007883       0.001053
## 
## 2. Quantiles for each variable:
## 
##              2.5%      25%     50%     75%   97.5%
## or        0.03107  0.07802  0.1244  0.1943  0.4312
## propdiff -0.68380 -0.54665 -0.4628 -0.3733 -0.1989
## rr        0.18648  0.31609  0.4053  0.5023  0.7137
## theta1    0.14673  0.24851  0.3150  0.3835  0.5251
## theta2    0.60608  0.72667  0.7834  0.8349  0.9089</code></pre>
</div>
<div id="normal-mean-known-variance" class="section level2">
<h2>Normal Mean, Known Variance</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n)
      {
         x[i]   ~   dnorm(mu,tau)    #  Likelihood function for each data point
       }
         mu     ~   dnorm(0,0.0001)  #  Prior for mu
         tau    &lt;- 1                 #  Prior for tau, actually a fixed value
         sigma  &lt;-  1/sqrt(tau)      #  Prior for sigma (as a function of tau)
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=c(-1.10635822,  0.56352639, -1.62101846,  0.06205707,  0.50183464,
0.45905694, -1.00045360, -0.58795638,  1.01602187, -0.26987089, 0.18354493 , 1.64605637, -0.96384666,  0.53842310, -1.11685831, 0.75908479 , 1.10442473 , -1.71124673, -0.42677894 , 0.68031412), n=20))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 1
##    Total graph size: 27
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;mu&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# History plots
traplot(output, parms=c(&quot;mu&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;mu&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 5001:15000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##      -0.064823       0.223872       0.002239       0.002239 
## 
## 2. Quantiles for each variable:
## 
##     2.5%      25%      50%      75%    97.5% 
## -0.49705 -0.21805 -0.06346  0.08899  0.37368</code></pre>
</div>
<div id="normal-mean-unknown-variance" class="section level2">
<h2>Normal Mean, Unknown Variance</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n)
      {
         x[i]   ~  dnorm(mu,tau)     #  Likelihood function for each data point
       }
         mu     ~  dnorm(0,0.0001)   #  Prior for mu
         tau    &lt;- 1/(sigma*sigma)   #  Prior for tau (as function of sigma)
         sigma  ~  dunif(0,20)       #  Prior for sigma
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=c( -1.10635822,  0.56352639, -1.62101846,  0.06205707,  0.50183464, 0.45905694,  -1.00045360, -0.58795638,  1.01602187, -0.26987089 , 0.18354493 , 1.64605637, -0.96384666,  0.53842310, -1.11685831, 0.75908479 , 1.10442473 , -1.71124673,  -0.42677894 , 0.68031412), n=20), inits = list(mu=1, sigma=1))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 2
##    Total graph size: 29
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;tau&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;mu&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;mu&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean     SD Naive SE Time-series SE
## mu    -0.06335 0.2302 0.002302       0.002332
## sigma  1.02784 0.1816 0.001816       0.002784
## tau    1.03138 0.3441 0.003441       0.004648
## 
## 2. Quantiles for each variable:
## 
##          2.5%     25%      50%     75%  97.5%
## mu    -0.5157 -0.2154 -0.06354 0.08746 0.3891
## sigma  0.7409  0.8997  1.00401 1.12867 1.4514
## tau    0.4747  0.7850  0.99204 1.23535 1.8217</code></pre>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n) {
     mu[i]  &lt;- alpha + b.sex*sex[i] + b.age*age[i]   # Regression function

     bp[i]   ~ dnorm(mu[i],tau)                      # Normal likelihood terms for each data point

    }
    alpha    ~ dnorm(0.0,1.0E-4)
    b.sex    ~ dnorm(0.0,1.0E-4)
    b.age    ~ dnorm(0.0,1.0E-4)
    tau     &lt;-  1/(sigma*sigma)                      # Prior for tau as function of sigma
    sigma    ~  dunif(0,20)                          # Prior directly on sigma
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)


#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(
  sex = c(0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
          1, 0, 1, 0, 0, 0, 1, 1, 1, 1),
  age = c(59, 52, 37, 40, 67, 43, 61, 34, 51, 58, 54, 31, 49, 45, 66, 48, 41, 47, 53, 62,
          60, 33, 44, 70, 56, 69, 35, 36, 68, 38),
  bp = c(143, 132, 88, 98, 177, 102, 154, 83, 131, 150, 131, 69, 111, 114, 170, 117, 96, 116, 131, 158,
         156, 75, 111, 184, 141, 182, 74, 87, 183, 89),
  n=30), list(alpha=50, b.sex=1, b.age=4))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 4
##    Total graph size: 163
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;, &quot;sigma&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean      SD  Naive SE Time-series SE
## alpha  -23.257 3.35454 0.0335454      0.2045617
## b.age    2.940 0.06104 0.0006104      0.0036951
## b.sex    1.618 1.54963 0.0154963      0.0345661
## mu[1]  150.175 1.06501 0.0106501      0.0172701
## mu[2]  131.216 1.16934 0.0116934      0.0191350
## mu[3]   85.506 1.36915 0.0136915      0.0644348
## mu[4]   94.324 1.24950 0.0124950      0.0544308
## mu[5]  173.691 1.32635 0.0132635      0.0425647
## mu[6]  104.760 1.15803 0.0115803      0.0177665
## mu[7]  156.054 1.11614 0.0111614      0.0224445
## mu[8]   78.305 1.38502 0.0138502      0.0466482
## mu[9]  128.277 1.15526 0.0115526      0.0176769
## mu[10] 147.236 1.04387 0.0104387      0.0133120
## mu[11] 137.095 1.20630 0.0120630      0.0276694
## mu[12]  67.868 1.64390 0.0164390      0.0876760
## mu[13] 120.780 1.01177 0.0101177      0.0236910
## mu[14] 109.022 1.08999 0.0108999      0.0376536
## mu[15] 170.752 1.28647 0.0128647      0.0390006
## mu[16] 119.458 1.13187 0.0113187      0.0151169
## mu[17]  97.264 1.21314 0.0121314      0.0512930
## mu[18] 116.518 1.13058 0.0113058      0.0150812
## mu[19] 132.538 0.98918 0.0098918      0.0124737
## mu[20] 158.994 1.14574 0.0114574      0.0254612
## mu[21] 154.732 1.37748 0.0137748      0.0492400
## mu[22]  73.747 1.54811 0.0154811      0.0797631
## mu[23] 107.700 1.14636 0.0114636      0.0158955
## mu[24] 182.510 1.45483 0.0145483      0.0515150
## mu[25] 141.357 1.01135 0.0101135      0.0124621
## mu[26] 179.570 1.41066 0.0141066      0.0490391
## mu[27]  81.244 1.35068 0.0135068      0.0429728
## mu[28]  84.184 1.31827 0.0131827      0.0393429
## mu[29] 178.248 1.70424 0.0170424      0.0703828
## mu[30]  90.063 1.25985 0.0125985      0.0323205
## sigma    4.070 0.58427 0.0058427      0.0087124
## tau      0.064 0.01763 0.0001763      0.0002487
## 
## 2. Quantiles for each variable:
## 
##             2.5%      25%       50%       75%    97.5%
## alpha  -29.99989 -25.4595 -23.19834 -21.05827 -16.9309
## b.age    2.82244   2.8988   2.93916   2.97931   3.0621
## b.sex   -1.44851   0.6010   1.61841   2.64870   4.6165
## mu[1]  148.03747 149.4757 150.18244 150.88885 152.2486
## mu[2]  128.93022 130.4266 131.22770 131.99120 133.4754
## mu[3]   82.80127  84.5894  85.53837  86.42530  88.1707
## mu[4]   91.85984  93.5006  94.34770  95.15974  96.7719
## mu[5]  171.07302 172.8273 173.70297 174.55797 176.2691
## mu[6]  102.48224 103.9925 104.76739 105.52607 107.0319
## mu[7]  153.82222 155.3184 156.06282 156.79740 158.2125
## mu[8]   75.57086  77.3685  78.29722  79.23601  81.0097
## mu[9]  126.02159 127.5004 128.28684 129.04231 130.5186
## mu[10] 145.14786 146.5546 147.24420 147.92925 149.2662
## mu[11] 134.72209 136.2832 137.10759 137.88484 139.4043
## mu[12]  64.59991  66.7736  67.92792  68.97213  71.0294
## mu[13] 118.77177 120.1192 120.78767 121.44709 122.7565
## mu[14] 106.85075 108.3022 109.02966 109.75117 111.1336
## mu[15] 168.20614 169.9113 170.76576 171.59611 173.2724
## mu[16] 117.25797 118.7136 119.46019 120.21387 121.6677
## mu[17]  94.86494  96.4631  97.28481  98.07645  99.6396
## mu[18] 114.32772 115.7702 116.51784 117.27283 118.7258
## mu[19] 130.55285 131.8916 132.54629 133.18610 134.4757
## mu[20] 156.70692 158.2370 159.00186 159.75277 161.2187
## mu[21] 152.03160 153.8009 154.74202 155.65106 157.4273
## mu[22]  70.67564  72.7131  73.80185  74.78890  76.7263
## mu[23] 105.45856 106.9431 107.70382 108.46212 109.9522
## mu[24] 179.61374 181.5560 182.51965 183.48001 185.3262
## mu[25] 139.32951 140.6995 141.36273 142.02222 143.3303
## mu[26] 176.76336 178.6456 179.58345 180.50266 182.3092
## mu[27]  78.59674  80.3324  81.23837  82.14296  83.8782
## mu[28]  81.58765  83.2979  84.17984  85.06209  86.7480
## mu[29] 174.88487 177.1036 178.24238 179.38519 181.5523
## mu[30]  87.56321  89.2221  90.06692  90.89917  92.5321
## sigma    3.12500   3.6582   4.00863   4.40243   5.4107
## tau      0.03416   0.0516   0.06223   0.07472   0.1024</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n) {

                                    #  Linear regression on logit
  logit(p[i]) &lt;- (alpha + b.sex*sex[i] + b.age*age[i])
  frac[i]   ~ dbern(p[i])

                                    #  Likelihood function for each data point
    }
    alpha     ~ dnorm(0.0,1.0E-4)   #  Prior for intercept
    b.sex     ~ dnorm(0.0,1.0E-4)   #  Prior for slope of sex
    b.age     ~ dnorm(0.0,1.0E-4)   #  Prior for slope of age
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

# data
dat = list(sex=c(1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
                 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
                 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
                 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
                 1, 1, 0, 1, 1, 1, 1, 1),
           age=c(69, 57, 61, 60, 69, 74, 63, 68, 64, 53, 60, 58, 79, 56, 53, 74,
                  56, 76, 72, 56, 66, 52, 77, 70, 69, 76, 72, 53, 69, 59, 73, 77, 55, 77,
                  68, 62, 56, 68, 70, 60, 65, 55, 64, 75, 60, 67, 61, 69, 75, 68, 72, 71,
                  54, 52, 54, 50, 75, 59, 65, 60, 60, 57, 51, 51, 63, 57, 80, 52, 65, 72,
                  80, 73, 76, 79, 66, 51, 76, 75, 66, 75, 78, 70, 67, 51, 70, 71, 71, 74,
                  74, 60, 58, 55, 61, 65, 52, 68, 75, 52, 53, 70),
           frac=c(1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
                  1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
                  0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
                  1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
                  0, 0, 1, 0, 0, 1),
           n=100)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=dat)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 100
##    Unobserved stochastic nodes: 3
##    Total graph size: 441
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;p&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean       SD  Naive SE Time-series SE
## alpha  -24.58771 3.968540 3.969e-02      0.7816492
## b.age    0.38762 0.061676 6.168e-04      0.0117434
## b.sex    1.54493 0.750605 7.506e-03      0.0379594
## p[1]     0.96925 0.022781 2.278e-04      0.0030637
## p[2]     0.28915 0.096922 9.692e-04      0.0033540
## p[3]     0.63911 0.104641 1.046e-03      0.0020681
## p[4]     0.22388 0.095682 9.568e-04      0.0057901
## p[5]     0.96925 0.022781 2.278e-04      0.0030637
## p[6]     0.99430 0.006213 6.213e-05      0.0009381
## p[7]     0.46076 0.123459 1.235e-03      0.0025275
## p[8]     0.84010 0.075910 7.591e-04      0.0031700
## p[9]     0.55138 0.122376 1.224e-03      0.0022635
## p[10]    0.02373 0.020632 2.063e-04      0.0028901
## p[11]    0.55072 0.110188 1.102e-03      0.0016589
## p[12]    0.36993 0.105876 1.059e-03      0.0020659
## p[13]    0.99889 0.001661 1.661e-05      0.0002545
## p[14]    0.22010 0.085331 8.533e-04      0.0043146
## p[15]    0.08717 0.049158 4.916e-04      0.0048513
## p[16]    0.97787 0.018811 1.881e-04      0.0020263
## p[17]    0.22010 0.085331 8.533e-04      0.0043146
## p[18]    0.98875 0.011243 1.124e-04      0.0013416
## p[19]    0.95643 0.031079 3.108e-04      0.0027884
## p[20]    0.06475 0.043362 4.336e-04      0.0048956
## p[21]    0.91597 0.046693 4.669e-04      0.0048720
## p[22]    0.06270 0.039405 3.940e-04      0.0044240
## p[23]    0.99788 0.002812 2.812e-05      0.0004196
## p[24]    0.91516 0.049945 4.994e-04      0.0033106
## p[25]    0.88284 0.062156 6.216e-04      0.0033404
## p[26]    0.99706 0.003662 3.662e-05      0.0005502
## p[27]    0.98886 0.010515 1.052e-04      0.0015217
## p[28]    0.02373 0.020632 2.063e-04      0.0028901
## p[29]    0.96925 0.022781 2.278e-04      0.0030637
## p[30]    0.45894 0.110619 1.106e-03      0.0016682
## p[31]    0.99204 0.008087 8.087e-05      0.0011609
## p[32]    0.99196 0.008673 8.673e-05      0.0011002
## p[33]    0.04645 0.034120 3.412e-04      0.0042287
## p[34]    0.99196 0.008673 8.673e-05      0.0011002
## p[35]    0.95687 0.029203 2.920e-04      0.0036545
## p[36]    0.71880 0.095043 9.504e-04      0.0034357
## p[37]    0.06475 0.043362 4.336e-04      0.0048956
## p[38]    0.84010 0.075910 7.591e-04      0.0031700
## p[39]    0.97809 0.017669 1.767e-04      0.0023426
## p[40]    0.55072 0.110188 1.102e-03      0.0016589
## p[41]    0.63877 0.115524 1.155e-03      0.0021387
## p[42]    0.16401 0.072728 7.273e-04      0.0049482
## p[43]    0.55138 0.122376 1.224e-03      0.0022635
## p[44]    0.98423 0.014557 1.456e-04      0.0016636
## p[45]    0.22388 0.095682 9.568e-04      0.0057901
## p[46]    0.93966 0.037134 3.713e-04      0.0042149
## p[47]    0.29271 0.108727 1.087e-03      0.0046188
## p[48]    0.88284 0.062156 6.216e-04      0.0033404
## p[49]    0.98423 0.014557 1.456e-04      0.0016636
## p[50]    0.95687 0.029203 2.920e-04      0.0036545
## p[51]    0.95643 0.031079 3.108e-04      0.0027884
## p[52]    0.98439 0.013649 1.365e-04      0.0019034
## p[53]    0.03323 0.026615 2.662e-04      0.0035396
## p[54]    0.06270 0.039405 3.940e-04      0.0044240
## p[55]    0.12026 0.060397 6.040e-04      0.0051470
## p[56]    0.03205 0.024551 2.455e-04      0.0032020
## p[57]    0.98423 0.014557 1.456e-04      0.0016636
## p[58]    0.16760 0.081348 8.135e-04      0.0062156
## p[59]    0.88387 0.057829 5.783e-04      0.0051009
## p[60]    0.55072 0.110188 1.102e-03      0.0016589
## p[61]    0.55072 0.110188 1.102e-03      0.0016589
## p[62]    0.28915 0.096922 9.692e-04      0.0033540
## p[63]    0.01212 0.012254 1.225e-04      0.0018256
## p[64]    0.01212 0.012254 1.225e-04      0.0018256
## p[65]    0.46076 0.123459 1.235e-03      0.0025275
## p[66]    0.28915 0.096922 9.692e-04      0.0033540
## p[67]    0.99919 0.001277 1.277e-05      0.0001945
## p[68]    0.06270 0.039405 3.940e-04      0.0044240
## p[69]    0.63877 0.115524 1.155e-03      0.0021387
## p[70]    0.95643 0.031079 3.108e-04      0.0027884
## p[71]    0.99919 0.001277 1.277e-05      0.0001945
## p[72]    0.99204 0.008087 8.087e-05      0.0011609
## p[73]    0.99706 0.003662 3.662e-05      0.0005502
## p[74]    0.99587 0.005156 5.156e-05      0.0006896
## p[75]    0.71782 0.104227 1.042e-03      0.0021515
## p[76]    0.01212 0.012254 1.225e-04      0.0018256
## p[77]    0.99706 0.003662 3.662e-05      0.0005502
## p[78]    0.99591 0.004770 4.770e-05      0.0007197
## p[79]    0.71782 0.104227 1.042e-03      0.0021515
## p[80]    0.98423 0.014557 1.456e-04      0.0016636
## p[81]    0.99424 0.006688 6.688e-05      0.0008680
## p[82]    0.91516 0.049945 4.994e-04      0.0033106
## p[83]    0.93966 0.037134 3.713e-04      0.0042149
## p[84]    0.01212 0.012254 1.225e-04      0.0018256
## p[85]    0.91516 0.049945 4.994e-04      0.0033106
## p[86]    0.98439 0.013649 1.365e-04      0.0019034
## p[87]    0.98439 0.013649 1.365e-04      0.0019034
## p[88]    0.99430 0.006213 6.213e-05      0.0009381
## p[89]    0.97787 0.018811 1.881e-04      0.0020263
## p[90]    0.55072 0.110188 1.102e-03      0.0016589
## p[91]    0.12340 0.067272 6.727e-04      0.0057752
## p[92]    0.16401 0.072728 7.273e-04      0.0049482
## p[93]    0.63911 0.104641 1.046e-03      0.0020681
## p[94]    0.88387 0.057829 5.783e-04      0.0051009
## p[95]    0.01695 0.015924 1.592e-04      0.0023143
## p[96]    0.95687 0.029203 2.920e-04      0.0036545
## p[97]    0.99591 0.004770 4.770e-05      0.0007197
## p[98]    0.06270 0.039405 3.940e-04      0.0044240
## p[99]    0.08717 0.049158 4.916e-04      0.0048513
## p[100]   0.97809 0.017669 1.767e-04      0.0023426
## 
## 2. Quantiles for each variable:
## 
##              2.5%        25%        50%       75%     97.5%
## alpha  -32.186733 -27.296514 -24.700475 -21.83703 -16.67183
## b.age    0.265427   0.345226   0.389136   0.42994   0.50583
## b.sex    0.110101   1.034191   1.539712   2.03995   3.05497
## p[1]     0.908200   0.960078   0.975659   0.98543   0.99453
## p[2]     0.128088   0.217901   0.279491   0.35151   0.49877
## p[3]     0.425622   0.566868   0.644909   0.71568   0.82560
## p[4]     0.076696   0.152741   0.209700   0.28128   0.44425
## p[5]     0.908200   0.960078   0.975659   0.98543   0.99453
## p[6]     0.975981   0.992836   0.996436   0.99824   0.99951
## p[7]     0.232525   0.372278   0.457081   0.54608   0.70453
## p[8]     0.659549   0.796359   0.852495   0.89640   0.95067
## p[9]     0.312960   0.465968   0.553521   0.63900   0.77998
## p[10]    0.003143   0.009818   0.017161   0.03064   0.08012
## p[11]    0.338645   0.472638   0.553221   0.62962   0.75631
## p[12]    0.182959   0.292826   0.363267   0.44113   0.58992
## p[13]    0.993855   0.998727   0.999487   0.99979   0.99996
## p[14]    0.086818   0.157206   0.208195   0.27212   0.41324
## p[15]    0.023986   0.051137   0.076428   0.11139   0.21164
## p[16]    0.927227   0.971430   0.983379   0.99050   0.99682
## p[17]    0.086818   0.157206   0.208195   0.27212   0.41324
## p[18]    0.957805   0.985732   0.992298   0.99586   0.99880
## p[19]    0.872971   0.943277   0.964667   0.97830   0.99177
## p[20]    0.013092   0.033749   0.053196   0.08408   0.17722
## p[21]    0.800912   0.891134   0.925904   0.95054   0.97781
## p[22]    0.015080   0.034301   0.053193   0.08064   0.16511
## p[23]    0.989392   0.997446   0.998889   0.99951   0.99989
## p[24]    0.787155   0.890657   0.925788   0.95147   0.97923
## p[25]    0.728360   0.849580   0.894812   0.92874   0.96734
## p[26]    0.986050   0.996403   0.998364   0.99925   0.99982
## p[27]    0.958903   0.985749   0.992326   0.99587   0.99870
## p[28]    0.003143   0.009818   0.017161   0.03064   0.08012
## p[29]    0.908200   0.960078   0.975659   0.98543   0.99453
## p[30]    0.254049   0.378834   0.457244   0.53570   0.67829
## p[31]    0.968606   0.989859   0.994756   0.99731   0.99920
## p[32]    0.968024   0.989965   0.994780   0.99729   0.99926
## p[33]    0.008128   0.022401   0.036708   0.06006   0.13683
## p[34]    0.968024   0.989965   0.994780   0.99729   0.99926
## p[35]    0.880753   0.943821   0.964530   0.97792   0.99115
## p[36]    0.516031   0.655922   0.727191   0.78879   0.87895
## p[37]    0.013092   0.033749   0.053196   0.08408   0.17722
## p[38]    0.659549   0.796359   0.852495   0.89640   0.95067
## p[39]    0.929766   0.971631   0.983459   0.99043   0.99660
## p[40]    0.338645   0.472638   0.553221   0.62962   0.75631
## p[41]    0.400095   0.561297   0.645628   0.72346   0.84336
## p[42]    0.057250   0.109869   0.151587   0.20522   0.33466
## p[43]    0.312960   0.465968   0.553521   0.63900   0.77998
## p[44]    0.944657   0.979794   0.988670   0.99371   0.99804
## p[45]    0.076696   0.152741   0.209700   0.28128   0.44425
## p[46]    0.844529   0.921615   0.948712   0.96701   0.98589
## p[47]    0.114102   0.211628   0.280616   0.36227   0.53166
## p[48]    0.728360   0.849580   0.894812   0.92874   0.96734
## p[49]    0.944657   0.979794   0.988670   0.99371   0.99804
## p[50]    0.880753   0.943821   0.964530   0.97792   0.99115
## p[51]    0.872971   0.943277   0.964667   0.97830   0.99177
## p[52]    0.946616   0.979923   0.988712   0.99371   0.99789
## p[53]    0.004993   0.014880   0.025145   0.04294   0.10500
## p[54]    0.015080   0.034301   0.053193   0.08064   0.16511
## p[55]    0.037633   0.075528   0.108581   0.15228   0.26822
## p[56]    0.005882   0.015147   0.025126   0.04112   0.09863
## p[57]    0.944657   0.979794   0.988670   0.99371   0.99804
## p[58]    0.050539   0.107494   0.152332   0.21406   0.36354
## p[59]    0.745061   0.851170   0.894777   0.92712   0.96534
## p[60]    0.338645   0.472638   0.553221   0.62962   0.75631
## p[61]    0.338645   0.472638   0.553221   0.62962   0.75631
## p[62]    0.128088   0.217901   0.279491   0.35151   0.49877
## p[63]    0.001183   0.004241   0.007921   0.01539   0.04699
## p[64]    0.001183   0.004241   0.007921   0.01539   0.04699
## p[65]    0.232525   0.372278   0.457081   0.54608   0.70453
## p[66]    0.128088   0.217901   0.279491   0.35151   0.49877
## p[67]    0.995320   0.999102   0.999652   0.99986   0.99997
## p[68]    0.015080   0.034301   0.053193   0.08064   0.16511
## p[69]    0.400095   0.561297   0.645628   0.72346   0.84336
## p[70]    0.872971   0.943277   0.964667   0.97830   0.99177
## p[71]    0.995320   0.999102   0.999652   0.99986   0.99997
## p[72]    0.968606   0.989859   0.994756   0.99731   0.99920
## p[73]    0.986050   0.996403   0.998364   0.99925   0.99982
## p[74]    0.981490   0.994999   0.997605   0.99884   0.99972
## p[75]    0.491164   0.650570   0.728140   0.79476   0.89052
## p[76]    0.001183   0.004241   0.007921   0.01539   0.04699
## p[77]    0.986050   0.996403   0.998364   0.99925   0.99982
## p[78]    0.981717   0.994921   0.997584   0.99885   0.99970
## p[79]    0.491164   0.650570   0.728140   0.79476   0.89052
## p[80]    0.944657   0.979794   0.988670   0.99371   0.99804
## p[81]    0.975593   0.992944   0.996456   0.99823   0.99954
## p[82]    0.787155   0.890657   0.925788   0.95147   0.97923
## p[83]    0.844529   0.921615   0.948712   0.96701   0.98589
## p[84]    0.001183   0.004241   0.007921   0.01539   0.04699
## p[85]    0.787155   0.890657   0.925788   0.95147   0.97923
## p[86]    0.946616   0.979923   0.988712   0.99371   0.99789
## p[87]    0.946616   0.979923   0.988712   0.99371   0.99789
## p[88]    0.975981   0.992836   0.996436   0.99824   0.99951
## p[89]    0.927227   0.971430   0.983379   0.99050   0.99682
## p[90]    0.338645   0.472638   0.553221   0.62962   0.75631
## p[91]    0.032735   0.074048   0.108645   0.15889   0.29074
## p[92]    0.057250   0.109869   0.151587   0.20522   0.33466
## p[93]    0.425622   0.566868   0.644909   0.71568   0.82560
## p[94]    0.745061   0.851170   0.894777   0.92712   0.96534
## p[95]    0.001943   0.006479   0.011674   0.02166   0.06128
## p[96]    0.880753   0.943821   0.964530   0.97792   0.99115
## p[97]    0.981717   0.994921   0.997584   0.99885   0.99970
## p[98]    0.015080   0.034301   0.053193   0.08064   0.16511
## p[99]    0.023986   0.051137   0.076428   0.11139   0.21164
## p[100]   0.929766   0.971631   0.983459   0.99043   0.99660</code></pre>
</div>
<div id="hierarchical-binomial-proportion" class="section level2">
<h2>Hierarchical Binomial Proportion</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:nmd) {                  #  nmd = number of MDs participating
        x[i] ~ dbin(p[i],n[i])          #  likelihood function for data for each MD

       logit(p[i]) &lt;- z[i]              #  Logit transform
        z[i] ~ dnorm(mu,tau)            #  Logit of probabilities follow normal distribution
      }

           mu ~   dnorm(0,0.001)        # Prior distribution for mu
          tau ~   dgamma(0.001,0.001)   # Prior distribution for tau
          y   ~   dnorm(mu, tau)        #  Predictive distribution for rate
        sigma &lt;-  1/sqrt(tau)           #  SD on the logit scale
            w &lt;-  exp(y)/(1+exp(y))     #  Predictive dist back on p-scale
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,list(n=c( 20, 6, 24, 13, 12, 4, 24, 12, 18),
                                        x=c( 19, 5, 22, 12, 11, 4, 23, 12, 16),
                                        nmd=9), list(mu=0, tau=1))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 9
##    Unobserved stochastic nodes: 12
##    Total graph size: 48
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;w&quot;, &quot;y&quot;, &quot;p&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;sigma&quot;, &quot;w&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;sigma&quot;, &quot;w&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean        SD  Naive SE Time-series SE
## mu      2.6735   0.37458 0.0037458       0.031377
## p[1]    0.9322   0.02680 0.0002680       0.001617
## p[2]    0.9254   0.03558 0.0003558       0.002268
## p[3]    0.9283   0.02829 0.0002829       0.001751
## p[4]    0.9287   0.03068 0.0003068       0.001803
## p[5]    0.9297   0.03008 0.0003008       0.001917
## p[6]    0.9313   0.03114 0.0003114       0.001911
## p[7]    0.9332   0.02615 0.0002615       0.001770
## p[8]    0.9344   0.02759 0.0002759       0.002051
## p[9]    0.9264   0.02973 0.0002973       0.001795
## sigma   0.2602   0.27315 0.0027315       0.017539
## tau   167.6830 364.64385 3.6464385      16.829703
## w       0.9285   0.03849 0.0003849       0.001850
## y       2.6755   0.52849 0.0052849       0.031270
## 
## 2. Quantiles for each variable:
## 
##         2.5%     25%     50%      75%     97.5%
## mu    1.9864 2.42555  2.6714   2.9063    3.4602
## p[1]  0.8703 0.91751  0.9365   0.9502    0.9746
## p[2]  0.8398 0.91125  0.9321   0.9473    0.9720
## p[3]  0.8593 0.91411  0.9331   0.9477    0.9700
## p[4]  0.8555 0.91466  0.9344   0.9486    0.9729
## p[5]  0.8597 0.91515  0.9348   0.9491    0.9752
## p[6]  0.8587 0.91690  0.9363   0.9509    0.9783
## p[7]  0.8731 0.91872  0.9371   0.9508    0.9756
## p[8]  0.8711 0.91978  0.9381   0.9522    0.9824
## p[9]  0.8541 0.91157  0.9321   0.9465    0.9688
## sigma 0.0278 0.08533  0.1720   0.3390    1.0214
## tau   0.9585 8.70330 33.7840 137.3381 1293.8410
## w     0.8479 0.91516  0.9352   0.9497    0.9785
## y     1.7186 2.37839  2.6701   2.9385    3.8196</code></pre>
</div>
