---
title: "Learning rjags using simple statistical models"
author: Nandini Dendukuri
date: '2020-06-28'
output: 
  html_document:
#    code_folding: hide
    theme: journal
    toc: yes
    toc_float: yes
    number_sections: false
categories:
  - Blogging
tags:
  - R Markdown
  - blogdown
  - rjags
  - mcmcplot
  - Bayesian analysis
---



<p>Here are some examples of fitting simple statistical models in rjags. They are useful for beginners learning Bayesian inference using the <a href="https://cran.r-project.org/web/packages/rjags/index.html">rjags</a> library. Notice in each case all that is required is specification of the probability density function for the observed data (i.e.Â the likelihood function) and the prior distribution functions for the unknown parameters.</p>
<p>Beyond that the user needs to be familiar with the rjags syntax for compiling the model, running the MCMC algorithm, sampling from the posterior distribution and examining the sample. The <a href="https://cran.r-project.org/web/packages/mcmcplots/index.html">mcmcplots</a> library is necessary for the last step.</p>
<p>These example programs were first written in WinBUGS by <a href="http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-675/CourseOutline.html">Lawrence Joseph</a>. They were reprogrammed using R Markdown by <a href="https://nandinidendukuri.com/team/mandy-yao/">Mandy Yao</a>.</p>
<div id="loading-the-necessary-libraries" class="section level2">
<h2>Loading the necessary libraries</h2>
<pre class="r"><code>library(rjags)
library(mcmcplots)</code></pre>
</div>
<div id="binomial-proportion" class="section level2">
<h2>Binomial Proportion</h2>
<pre class="r"><code>modelString = 
  &quot;model { 
  
    x~dbin(theta,n)        #  Likelihood function

    theta ~ dbeta(1,1)     #  Prior density for theta
    
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=6,n=20))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1
##    Unobserved stochastic nodes: 1
##    Total graph size: 4
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update(jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;theta&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# Density plots
denplot(output, parms=c(&quot;theta&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># History plot(s)
traplot(output, parms=c(&quot;theta&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##      0.3169867      0.0960600      0.0009606      0.0012131 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## 0.1460 0.2477 0.3115 0.3799 0.5177</code></pre>
</div>
<div id="binomial-proportion-difference" class="section level2">
<h2>Binomial Proportion Difference</h2>
<pre class="r"><code>modelString = 
  &quot;model { x  ~ dbin(theta1, n1)   #  Likelihood for group 1
         theta1 ~ dbeta(1,1)         #  Prior for theta1

         y  ~ dbin(theta2, n2)   #  Likelihood for group 2
         theta2 ~ dbeta(1,1)         #  Prior for theta2

      propdiff &lt;- theta1-theta2        #  Calculate difference for binomial parameters
      rr &lt;- theta1/theta2              #  Calculate relative risk
                                       #  Calculate odds ratio
      or&lt;- theta1*(1-theta2)/((1-theta1)*theta2)
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x  = 6,
                                             n1 = 20,
                                             y  = 20,
                                             n2 = 25))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 2
##    Unobserved stochastic nodes: 2
##    Total graph size: 14
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;theta1&quot;, &quot;theta2&quot;, &quot;propdiff&quot;, &quot;or&quot;, &quot;rr&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# History plots
traplot(output, parms=c(&quot;propdiff&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;propdiff&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean      SD  Naive SE Time-series SE
## or        0.1517 0.11273 0.0011273       0.001505
## propdiff -0.4580 0.12442 0.0012442       0.001599
## rr        0.4141 0.13503 0.0013503       0.001738
## theta1    0.3174 0.09599 0.0009599       0.001228
## theta2    0.7755 0.07937 0.0007937       0.001049
## 
## 2. Quantiles for each variable:
## 
##              2.5%      25%     50%     75%   97.5%
## or        0.03194  0.07896  0.1240  0.1900  0.4341
## propdiff -0.68140 -0.54492 -0.4633 -0.3796 -0.1998
## rr        0.18517  0.31847  0.4028  0.4980  0.7050
## theta1    0.14598  0.24862  0.3130  0.3809  0.5157
## theta2    0.59996  0.72645  0.7832  0.8330  0.9084</code></pre>
</div>
<div id="normal-mean-known-variance" class="section level2">
<h2>Normal Mean, Known Variance</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n)
      {
         x[i]   ~   dnorm(mu,tau)    #  Likelihood function for each data point
       }
         mu     ~   dnorm(0,0.0001)  #  Prior for mu
         tau    &lt;- 1                 #  Prior for tau, actually a fixed value
         sigma  &lt;-  1/sqrt(tau)      #  Prior for sigma (as a function of tau)
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=c(-1.10635822,  0.56352639, -1.62101846,  0.06205707,  0.50183464,
0.45905694, -1.00045360, -0.58795638,  1.01602187, -0.26987089, 0.18354493 , 1.64605637, -0.96384666,  0.53842310, -1.11685831, 0.75908479 , 1.10442473 , -1.71124673, -0.42677894 , 0.68031412), n=20))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 1
##    Total graph size: 27
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c( &quot;mu&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample
# History plots
traplot(output, parms=c(&quot;mu&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;mu&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 5001:15000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##      -0.063399       0.225890       0.002259       0.002259 
## 
## 2. Quantiles for each variable:
## 
##     2.5%      25%      50%      75%    97.5% 
## -0.50886 -0.21393 -0.06442  0.08799  0.37739</code></pre>
</div>
<div id="normal-mean-unknown-variance" class="section level2">
<h2>Normal Mean, Unknown Variance</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n)
      {
         x[i]   ~  dnorm(mu,tau)     #  Likelihood function for each data point
       }
         mu     ~  dnorm(0,0.0001)   #  Prior for mu
         tau    &lt;- 1/(sigma*sigma)   #  Prior for tau (as function of sigma)
         sigma  ~  dunif(0,20)       #  Prior for sigma
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(x=c( -1.10635822,  0.56352639, -1.62101846,  0.06205707,  0.50183464, 0.45905694,  -1.00045360, -0.58795638,  1.01602187, -0.26987089 , 0.18354493 , 1.64605637, -0.96384666,  0.53842310, -1.11685831, 0.75908479 , 1.10442473 , -1.71124673,  -0.42677894 , 0.68031412), n=20), inits = list(mu=1, sigma=1))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 2
##    Total graph size: 29
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;sigma&quot;, &quot;tau&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;mu&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;mu&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##          Mean     SD Naive SE Time-series SE
## mu    -0.0655 0.2347 0.002347       0.002347
## sigma  1.0301 0.1825 0.001825       0.002637
## tau    1.0261 0.3366 0.003366       0.004433
## 
## 2. Quantiles for each variable:
## 
##          2.5%     25%      50%     75%  97.5%
## mu    -0.5271 -0.2190 -0.06318 0.08613 0.3971
## sigma  0.7495  0.9005  1.00474 1.13078 1.4586
## tau    0.4700  0.7821  0.99058 1.23331 1.7800</code></pre>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n) {
     mu[i]  &lt;- alpha + b.sex*sex[i] + b.age*age[i]   # Regression function

     bp[i]   ~ dnorm(mu[i],tau)                      # Normal likelihood terms for each data point

    }
    alpha    ~ dnorm(0.0,1.0E-4)
    b.sex    ~ dnorm(0.0,1.0E-4)
    b.age    ~ dnorm(0.0,1.0E-4)
    tau     &lt;-  1/(sigma*sigma)                      # Prior for tau as function of sigma
    sigma    ~  dunif(0,20)                          # Prior directly on sigma
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)


#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=list(
  sex = c(0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
          1, 0, 1, 0, 0, 0, 1, 1, 1, 1),
  age = c(59, 52, 37, 40, 67, 43, 61, 34, 51, 58, 54, 31, 49, 45, 66, 48, 41, 47, 53, 62,
          60, 33, 44, 70, 56, 69, 35, 36, 68, 38),
  bp = c(143, 132, 88, 98, 177, 102, 154, 83, 131, 150, 131, 69, 111, 114, 170, 117, 96, 116, 131, 158,
         156, 75, 111, 184, 141, 182, 74, 87, 183, 89),
  n=30), list(alpha=50, b.sex=1, b.age=4))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 4
##    Total graph size: 163
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;, &quot;sigma&quot;)  

#Sampling from the posterior distribution:
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;tau&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean      SD  Naive SE Time-series SE
## alpha  -23.30284 3.54834 0.0354834      0.2279437
## b.age    2.94043 0.06493 0.0006493      0.0040342
## b.sex    1.63420 1.55943 0.0155943      0.0361753
## mu[1]  150.18255 1.08686 0.0108686      0.0186462
## mu[2]  131.23374 1.19203 0.0119203      0.0227472
## mu[3]   85.49309 1.41553 0.0141553      0.0717312
## mu[4]   94.31438 1.28501 0.0128501      0.0612546
## mu[5]  173.70600 1.37625 0.0137625      0.0499959
## mu[6]  104.76986 1.17782 0.0117782      0.0175499
## mu[7]  156.06342 1.14406 0.0114406      0.0269706
## mu[8]   78.30599 1.42709 0.0142709      0.0531912
## mu[9]  128.29331 1.17621 0.0117621      0.0171801
## mu[10] 147.24212 1.06307 0.0106307      0.0158882
## mu[11] 137.11460 1.23336 0.0123336      0.0300647
## mu[12]  67.85050 1.71357 0.0171357      0.0970822
## mu[13] 120.77825 1.02429 0.0102429      0.0258858
## mu[14] 109.01653 1.11015 0.0111015      0.0413520
## mu[15] 170.76557 1.33246 0.0133246      0.0474240
## mu[16] 119.47202 1.14963 0.0114963      0.0150707
## mu[17]  97.25481 1.24524 0.0124524      0.0573331
## mu[18] 116.53159 1.14800 0.0114800      0.0150394
## mu[19] 132.53997 1.00041 0.0100041      0.0133727
## mu[20] 159.00385 1.17700 0.0117700      0.0308338
## mu[21] 154.75718 1.42276 0.0142276      0.0538401
## mu[22]  73.73136 1.60988 0.0160988      0.0879788
## mu[23] 107.71029 1.16502 0.0116502      0.0159725
## mu[24] 182.52729 1.51674 0.0151674      0.0607699
## mu[25] 141.36126 1.02621 0.0102621      0.0125786
## mu[26] 179.58686 1.46853 0.0146853      0.0564043
## mu[27]  81.24642 1.38950 0.0138950      0.0487828
## mu[28]  84.18685 1.35399 0.0135399      0.0444338
## mu[29] 178.28062 1.77967 0.0177967      0.0900810
## mu[30]  90.06771 1.28984 0.0128984      0.0365972
## sigma    4.06310 0.57930 0.0057930      0.0091988
## tau      0.06419 0.01773 0.0001773      0.0002615
## 
## 2. Quantiles for each variable:
## 
##             2.5%       25%       50%       75%    97.5%
## alpha  -30.30561 -25.66787 -23.32380 -20.91159 -16.2315
## b.age    2.81014   2.89682   2.94055   2.98325   3.0689
## b.sex   -1.39800   0.58930   1.63577   2.67239   4.6979
## mu[1]  148.00378 149.47470 150.19709 150.90468 152.2792
## mu[2]  128.87995 130.43883 131.23984 132.02987 133.5454
## mu[3]   82.71375  84.53997  85.49344  86.43789  88.2891
## mu[4]   91.77282  93.44111  94.31328  95.17421  96.8469
## mu[5]  170.95905 172.79707 173.71137 174.62863 176.3477
## mu[6]  102.44054 103.99182 104.77012 105.56109 107.0633
## mu[7]  153.79010 155.31397 156.07779 156.82749 158.2471
## mu[8]   75.44137  77.37066  78.30070  79.25051  81.1092
## mu[9]  125.96615 127.51050 128.29819 129.08662 130.5923
## mu[10] 145.09764 146.54844 147.25338 147.94454 149.2910
## mu[11] 134.68654 136.29641 137.12069 137.94924 139.5113
## mu[12]  64.45375  66.69833  67.84084  69.01186  71.2172
## mu[13] 118.71798 120.10031 120.79641 121.46810 122.7813
## mu[14] 106.81725 108.26155 109.02119 109.76772 111.1875
## mu[15] 168.11714 169.88727 170.77169 171.65860 173.3183
## mu[16] 117.21743 118.70415 119.47681 120.25778 121.7333
## mu[17]  94.80160  96.40245  97.25655  98.08872  99.7098
## mu[18] 114.28595 115.76636 116.53733 117.31011 118.7699
## mu[19] 130.52693 131.88809 132.54852 133.20266 134.4734
## mu[20] 156.66781 158.23547 159.01905 159.79153 161.2489
## mu[21] 152.00068 153.81928 154.76163 155.70763 157.5690
## mu[22]  70.56419  72.64881  73.72183  74.81211  76.9004
## mu[23] 105.40261 106.93969 107.71400 108.49484 109.9959
## mu[24] 179.50113 181.52510 182.53295 183.54632 185.4283
## mu[25] 139.29800 140.69520 141.36765 142.03656 143.3349
## mu[26] 176.66573 178.61935 179.59201 180.57111 182.4064
## mu[27]  78.44616  80.33973  81.23942  82.17177  83.9796
## mu[28]  81.45846  83.30920  84.17981  85.09405  86.8478
## mu[29] 174.78513 177.09652 178.28078 179.45514 181.8071
## mu[30]  87.47054  89.22048  90.06852  90.92796  92.6011
## sigma    3.11316   3.65438   4.00060   4.40443   5.3960
## tau      0.03434   0.05155   0.06248   0.07488   0.1032</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:n) {

                                    #  Linear regression on logit
  logit(p[i]) &lt;- (alpha + b.sex*sex[i] + b.age*age[i])
  frac[i]   ~ dbern(p[i])

                                    #  Likelihood function for each data point
    }
    alpha     ~ dnorm(0.0,1.0E-4)   #  Prior for intercept
    b.sex     ~ dnorm(0.0,1.0E-4)   #  Prior for slope of sex
    b.age     ~ dnorm(0.0,1.0E-4)   #  Prior for slope of age
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

# data
dat = list(sex=c(1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
                 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
                 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
                 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
                 1, 1, 0, 1, 1, 1, 1, 1),
           age=c(69, 57, 61, 60, 69, 74, 63, 68, 64, 53, 60, 58, 79, 56, 53, 74,
                  56, 76, 72, 56, 66, 52, 77, 70, 69, 76, 72, 53, 69, 59, 73, 77, 55, 77,
                  68, 62, 56, 68, 70, 60, 65, 55, 64, 75, 60, 67, 61, 69, 75, 68, 72, 71,
                  54, 52, 54, 50, 75, 59, 65, 60, 60, 57, 51, 51, 63, 57, 80, 52, 65, 72,
                  80, 73, 76, 79, 66, 51, 76, 75, 66, 75, 78, 70, 67, 51, 70, 71, 71, 74,
                  74, 60, 58, 55, 61, 65, 52, 68, 75, 52, 53, 70),
           frac=c(1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
                  1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
                  0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
                  1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
                  0, 0, 1, 0, 0, 1),
           n=100)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,data=dat)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 100
##    Unobserved stochastic nodes: 3
##    Total graph size: 441
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;, &quot;p&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;alpha&quot;, &quot;b.age&quot;, &quot;b.sex&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean       SD  Naive SE Time-series SE
## alpha  -25.42038 4.757198 4.757e-02      1.1061711
## b.age    0.40044 0.073909 7.391e-04      0.0177446
## b.sex    1.62555 0.792969 7.930e-03      0.0448025
## p[1]     0.97117 0.023985 2.398e-04      0.0040599
## p[2]     0.28543 0.097872 9.787e-04      0.0043515
## p[3]     0.64531 0.105068 1.051e-03      0.0029810
## p[4]     0.21510 0.097218 9.722e-04      0.0068370
## p[5]     0.97117 0.023985 2.398e-04      0.0040599
## p[6]     0.99456 0.006816 6.816e-05      0.0011273
## p[7]     0.45497 0.124931 1.249e-03      0.0026809
## p[8]     0.84402 0.075814 7.581e-04      0.0038256
## p[9]     0.54836 0.123638 1.236e-03      0.0025068
## p[10]    0.02243 0.022303 2.230e-04      0.0033722
## p[11]    0.55459 0.110701 1.107e-03      0.0017084
## p[12]    0.36818 0.106688 1.067e-03      0.0022453
## p[13]    0.99890 0.001889 1.889e-05      0.0002910
## p[14]    0.21542 0.086404 8.640e-04      0.0057811
## p[15]    0.08357 0.050360 5.036e-04      0.0064872
## p[16]    0.97904 0.018895 1.890e-04      0.0025423
## p[17]    0.21542 0.086404 8.640e-04      0.0057811
## p[18]    0.98935 0.011326 1.133e-04      0.0016253
## p[19]    0.95855 0.031109 3.111e-04      0.0036043
## p[20]    0.06092 0.045444 4.544e-04      0.0061408
## p[21]    0.92076 0.047888 4.789e-04      0.0066012
## p[22]    0.05988 0.040530 4.053e-04      0.0056136
## p[23]    0.99793 0.003154 3.154e-05      0.0005009
## p[24]    0.91849 0.049849 4.985e-04      0.0043364
## p[25]    0.88664 0.062019 6.202e-04      0.0043481
## p[26]    0.99715 0.004078 4.078e-05      0.0006570
## p[27]    0.98948 0.011357 1.136e-04      0.0019128
## p[28]    0.02243 0.022303 2.230e-04      0.0033722
## p[29]    0.97117 0.023985 2.398e-04      0.0040599
## p[30]    0.45992 0.111279 1.113e-03      0.0017219
## p[31]    0.99245 0.008804 8.804e-05      0.0014715
## p[32]    0.99238 0.008747 8.747e-05      0.0012704
## p[33]    0.04370 0.036139 3.614e-04      0.0052293
## p[34]    0.99238 0.008747 8.747e-05      0.0012704
## p[35]    0.95955 0.030473 3.047e-04      0.0049593
## p[36]    0.72631 0.095499 9.550e-04      0.0045816
## p[37]    0.06092 0.045444 4.544e-04      0.0061408
## p[38]    0.84402 0.075814 7.581e-04      0.0038256
## p[39]    0.97944 0.018767 1.877e-04      0.0031967
## p[40]    0.55459 0.110701 1.107e-03      0.0017084
## p[41]    0.63857 0.116402 1.164e-03      0.0024013
## p[42]    0.15922 0.073898 7.390e-04      0.0066674
## p[43]    0.54836 0.123638 1.236e-03      0.0025068
## p[44]    0.98508 0.014645 1.464e-04      0.0020514
## p[45]    0.21510 0.097218 9.722e-04      0.0068370
## p[46]    0.94331 0.038406 3.841e-04      0.0058568
## p[47]    0.28383 0.110214 1.102e-03      0.0057834
## p[48]    0.88664 0.062019 6.202e-04      0.0043481
## p[49]    0.98508 0.014645 1.464e-04      0.0020514
## p[50]    0.95955 0.030473 3.047e-04      0.0049593
## p[51]    0.95855 0.031109 3.111e-04      0.0036043
## p[52]    0.98531 0.014622 1.462e-04      0.0024696
## p[53]    0.03130 0.028486 2.849e-04      0.0043100
## p[54]    0.05988 0.040530 4.053e-04      0.0056136
## p[55]    0.11594 0.061616 6.162e-04      0.0068670
## p[56]    0.03051 0.025414 2.541e-04      0.0038556
## p[57]    0.98508 0.014645 1.464e-04      0.0020514
## p[58]    0.15973 0.083037 8.304e-04      0.0074614
## p[59]    0.88987 0.058865 5.886e-04      0.0069848
## p[60]    0.55459 0.110701 1.107e-03      0.0017084
## p[61]    0.55459 0.110701 1.107e-03      0.0017084
## p[62]    0.28543 0.097872 9.787e-04      0.0043515
## p[63]    0.01156 0.013488 1.349e-04      0.0020629
## p[64]    0.01156 0.013488 1.349e-04      0.0020629
## p[65]    0.45497 0.124931 1.249e-03      0.0026809
## p[66]    0.28543 0.097872 9.787e-04      0.0043515
## p[67]    0.99919 0.001463 1.463e-05      0.0002175
## p[68]    0.05988 0.040530 4.053e-04      0.0056136
## p[69]    0.63857 0.116402 1.164e-03      0.0024013
## p[70]    0.95855 0.031109 3.111e-04      0.0036043
## p[71]    0.99919 0.001463 1.463e-05      0.0002175
## p[72]    0.99245 0.008804 8.804e-05      0.0014715
## p[73]    0.99715 0.004078 4.078e-05      0.0006570
## p[74]    0.99606 0.005210 5.210e-05      0.0007291
## p[75]    0.71987 0.104667 1.047e-03      0.0022644
## p[76]    0.01156 0.013488 1.349e-04      0.0020629
## p[77]    0.99715 0.004078 4.078e-05      0.0006570
## p[78]    0.99607 0.005273 5.273e-05      0.0008613
## p[79]    0.71987 0.104667 1.047e-03      0.0022644
## p[80]    0.98508 0.014645 1.464e-04      0.0020514
## p[81]    0.99453 0.006751 6.751e-05      0.0009453
## p[82]    0.91849 0.049849 4.985e-04      0.0043364
## p[83]    0.94331 0.038406 3.841e-04      0.0058568
## p[84]    0.01156 0.013488 1.349e-04      0.0020629
## p[85]    0.91849 0.049849 4.985e-04      0.0043364
## p[86]    0.98531 0.014622 1.462e-04      0.0024696
## p[87]    0.98531 0.014622 1.462e-04      0.0024696
## p[88]    0.99456 0.006816 6.816e-05      0.0011273
## p[89]    0.97904 0.018895 1.890e-04      0.0025423
## p[90]    0.55459 0.110701 1.107e-03      0.0017084
## p[91]    0.11687 0.069158 6.916e-04      0.0074936
## p[92]    0.15922 0.073898 7.390e-04      0.0066674
## p[93]    0.64531 0.105068 1.051e-03      0.0029810
## p[94]    0.88987 0.058865 5.886e-04      0.0069848
## p[95]    0.01609 0.017375 1.738e-04      0.0026519
## p[96]    0.95955 0.030473 3.047e-04      0.0049593
## p[97]    0.99607 0.005273 5.273e-05      0.0008613
## p[98]    0.05988 0.040530 4.053e-04      0.0056136
## p[99]    0.08357 0.050360 5.036e-04      0.0064872
## p[100]   0.97944 0.018767 1.877e-04      0.0031967
## 
## 2. Quantiles for each variable:
## 
##              2.5%        25%        50%       75%     97.5%
## alpha  -3.421e+01 -28.920411 -25.537423 -21.98795 -16.44897
## b.age   2.601e-01   0.346310   0.401764   0.45479   0.53835
## b.sex   1.462e-01   1.082326   1.593417   2.15304   3.24755
## p[1]    9.059e-01   0.961866   0.978514   0.98799   0.99605
## p[2]    1.217e-01   0.212482   0.276336   0.34973   0.49858
## p[3]    4.297e-01   0.574067   0.650143   0.72155   0.83562
## p[4]    6.731e-02   0.141608   0.201111   0.27376   0.44189
## p[5]    9.059e-01   0.961866   0.978514   0.98799   0.99605
## p[6]    9.742e-01   0.993217   0.997056   0.99872   0.99971
## p[7]    2.246e-01   0.365973   0.452384   0.54135   0.70568
## p[8]    6.657e-01   0.799868   0.855835   0.90041   0.95564
## p[9]    3.070e-01   0.461916   0.550202   0.63693   0.78236
## p[10]   2.190e-03   0.007719   0.014931   0.02878   0.08640
## p[11]   3.401e-01   0.477344   0.556044   0.63369   0.76376
## p[12]   1.800e-01   0.289252   0.362502   0.44071   0.58732
## p[13]   9.933e-01   0.998813   0.999602   0.99987   0.99998
## p[14]   7.966e-02   0.150484   0.203925   0.26897   0.41139
## p[15]   1.943e-02   0.046394   0.071672   0.10921   0.20986
## p[16]   9.278e-01   0.972132   0.984807   0.99207   0.99773
## p[17]   7.966e-02   0.150484   0.203925   0.26897   0.41139
## p[18]   9.575e-01   0.986273   0.993094   0.99675   0.99918
## p[19]   8.777e-01   0.944743   0.966836   0.98086   0.99372
## p[20]   1.002e-02   0.028697   0.048092   0.07999   0.18375
## p[21]   8.027e-01   0.896323   0.931581   0.95618   0.98213
## p[22]   1.182e-02   0.030430   0.049180   0.07881   0.16488
## p[23]   9.885e-01   0.997607   0.999114   0.99967   0.99994
## p[24]   7.956e-01   0.892415   0.929468   0.95525   0.98321
## p[25]   7.364e-01   0.852231   0.898333   0.93275   0.97268
## p[26]   9.850e-01   0.996615   0.998670   0.99948   0.99990
## p[27]   9.562e-01   0.986452   0.993456   0.99686   0.99917
## p[28]   2.190e-03   0.007719   0.014931   0.02878   0.08640
## p[29]   9.059e-01   0.961866   0.978514   0.98799   0.99605
## p[30]   2.526e-01   0.379578   0.458133   0.53759   0.67988
## p[31]   9.662e-01   0.990410   0.995608   0.99799   0.99951
## p[32]   9.673e-01   0.990291   0.995351   0.99792   0.99951
## p[33]   6.059e-03   0.018557   0.032725   0.05725   0.14394
## p[34]   9.673e-01   0.990291   0.995351   0.99792   0.99951
## p[35]   8.797e-01   0.946480   0.968304   0.98139   0.99341
## p[36]   5.223e-01   0.663255   0.733393   0.79716   0.88940
## p[37]   1.002e-02   0.028697   0.048092   0.07999   0.18375
## p[38]   6.657e-01   0.799868   0.855835   0.90041   0.95564
## p[39]   9.266e-01   0.972915   0.985495   0.99227   0.99763
## p[40]   3.401e-01   0.477344   0.556044   0.63369   0.76376
## p[41]   3.999e-01   0.560665   0.644089   0.72358   0.84608
## p[42]   5.097e-02   0.103813   0.146562   0.20260   0.33265
## p[43]   3.070e-01   0.461916   0.550202   0.63693   0.78236
## p[44]   9.445e-01   0.980450   0.989772   0.99492   0.99864
## p[45]   6.731e-02   0.141608   0.201111   0.27376   0.44189
## p[46]   8.452e-01   0.925167   0.953314   0.97132   0.98912
## p[47]   1.040e-01   0.201541   0.271737   0.35409   0.52668
## p[48]   7.364e-01   0.852231   0.898333   0.93275   0.97268
## p[49]   9.445e-01   0.980450   0.989772   0.99492   0.99864
## p[50]   8.797e-01   0.946480   0.968304   0.98139   0.99341
## p[51]   8.777e-01   0.944743   0.966836   0.98086   0.99372
## p[52]   9.432e-01   0.980787   0.990211   0.99507   0.99859
## p[53]   3.681e-03   0.011995   0.022090   0.04066   0.11228
## p[54]   1.182e-02   0.030430   0.049180   0.07881   0.16488
## p[55]   3.177e-02   0.070247   0.103443   0.15000   0.26577
## p[56]   4.367e-03   0.012751   0.022629   0.04023   0.10033
## p[57]   9.445e-01   0.980450   0.989772   0.99492   0.99864
## p[58]   4.294e-02   0.097385   0.143921   0.20670   0.35985
## p[59]   7.487e-01   0.857568   0.901436   0.93362   0.97104
## p[60]   3.401e-01   0.477344   0.556044   0.63369   0.76376
## p[61]   3.401e-01   0.477344   0.556044   0.63369   0.76376
## p[62]   1.217e-01   0.212482   0.276336   0.34973   0.49858
## p[63]   7.875e-04   0.003171   0.006702   0.01440   0.05091
## p[64]   7.875e-04   0.003171   0.006702   0.01440   0.05091
## p[65]   2.246e-01   0.365973   0.452384   0.54135   0.70568
## p[66]   1.217e-01   0.212482   0.276336   0.34973   0.49858
## p[67]   9.949e-01   0.999168   0.999734   0.99992   0.99999
## p[68]   1.182e-02   0.030430   0.049180   0.07881   0.16488
## p[69]   3.999e-01   0.560665   0.644089   0.72358   0.84608
## p[70]   8.777e-01   0.944743   0.966836   0.98086   0.99372
## p[71]   9.949e-01   0.999168   0.999734   0.99992   0.99999
## p[72]   9.662e-01   0.990410   0.995608   0.99799   0.99951
## p[73]   9.850e-01   0.996615   0.998670   0.99948   0.99990
## p[74]   9.808e-01   0.995194   0.997896   0.99915   0.99982
## p[75]   4.985e-01   0.651738   0.728945   0.79688   0.89401
## p[76]   7.875e-04   0.003171   0.006702   0.01440   0.05091
## p[77]   9.850e-01   0.996615   0.998670   0.99948   0.99990
## p[78]   9.803e-01   0.995196   0.998023   0.99918   0.99983
## p[79]   4.985e-01   0.651738   0.728945   0.79688   0.89401
## p[80]   9.445e-01   0.980450   0.989772   0.99492   0.99864
## p[81]   9.750e-01   0.993172   0.996873   0.99867   0.99971
## p[82]   7.956e-01   0.892415   0.929468   0.95525   0.98321
## p[83]   8.452e-01   0.925167   0.953314   0.97132   0.98912
## p[84]   7.875e-04   0.003171   0.006702   0.01440   0.05091
## p[85]   7.956e-01   0.892415   0.929468   0.95525   0.98321
## p[86]   9.432e-01   0.980787   0.990211   0.99507   0.99859
## p[87]   9.432e-01   0.980787   0.990211   0.99507   0.99859
## p[88]   9.742e-01   0.993217   0.997056   0.99872   0.99971
## p[89]   9.278e-01   0.972132   0.984807   0.99207   0.99773
## p[90]   3.401e-01   0.477344   0.556044   0.63369   0.76376
## p[91]   2.647e-02   0.065815   0.101069   0.15267   0.28900
## p[92]   5.097e-02   0.103813   0.146562   0.20260   0.33265
## p[93]   4.297e-01   0.574067   0.650143   0.72155   0.83562
## p[94]   7.487e-01   0.857568   0.901436   0.93362   0.97104
## p[95]   1.316e-03   0.004937   0.010010   0.02034   0.06594
## p[96]   8.797e-01   0.946480   0.968304   0.98139   0.99341
## p[97]   9.803e-01   0.995196   0.998023   0.99918   0.99983
## p[98]   1.182e-02   0.030430   0.049180   0.07881   0.16488
## p[99]   1.943e-02   0.046394   0.071672   0.10921   0.20986
## p[100]  9.266e-01   0.972915   0.985495   0.99227   0.99763</code></pre>
</div>
<div id="hierarchical-binomial-proportion" class="section level2">
<h2>Hierarchical Binomial Proportion</h2>
<pre class="r"><code>modelString = 
  &quot;model { for (i in 1:nmd) {                  #  nmd = number of MDs participating
        x[i] ~ dbin(p[i],n[i])          #  likelihood function for data for each MD

       logit(p[i]) &lt;- z[i]              #  Logit transform
        z[i] ~ dnorm(mu,tau)            #  Logit of probabilities follow normal distribution
      }

           mu ~   dnorm(0,0.001)        # Prior distribution for mu
          tau ~   dgamma(0.001,0.001)   # Prior distribution for tau
          y   ~   dnorm(mu, tau)        #  Predictive distribution for rate
        sigma &lt;-  1/sqrt(tau)           #  SD on the logit scale
            w &lt;-  exp(y)/(1+exp(y))     #  Predictive dist back on p-scale
  }&quot;

#Write the model to a file
writeLines(modelString,con=&quot;model.txt&quot;)

#Compiling the model together with the data
jagsModel = jags.model(&quot;model.txt&quot;,list(n=c( 20, 6, 24, 13, 12, 4, 24, 12, 18),
                                        x=c( 19, 5, 22, 12, 11, 4, 23, 12, 16),
                                        nmd=9), list(mu=0, tau=1))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 9
##    Unobserved stochastic nodes: 12
##    Total graph size: 48
## 
## Initializing model</code></pre>
<pre class="r"><code>#Burn-in
update( jagsModel,n.iter=5000)

# The parameter(s) to be monitored.
parameters = c(&quot;mu&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;w&quot;, &quot;y&quot;, &quot;p&quot;)  

#Sampling from the posterior distribution
output = coda.samples(jagsModel,variable.names=parameters,n.iter=10000)

# Examining the posterior sample

# History plots
traplot(output, parms=c(&quot;sigma&quot;, &quot;w&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Density plots
denplot(output, parms=c(&quot;sigma&quot;, &quot;w&quot;))</code></pre>
<p><img src="/post/Simple-Models_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code># Summary statistics 
summary(output) </code></pre>
<pre><code>## 
## Iterations = 6001:16000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean        SD  Naive SE Time-series SE
## mu      2.6884   0.39740 0.0039740       0.040950
## p[1]    0.9333   0.02618 0.0002618       0.002223
## p[2]    0.9276   0.03512 0.0003512       0.002177
## p[3]    0.9302   0.02791 0.0002791       0.002239
## p[4]    0.9298   0.02872 0.0002872       0.002296
## p[5]    0.9292   0.03116 0.0003116       0.002332
## p[6]    0.9324   0.02889 0.0002889       0.002292
## p[7]    0.9336   0.02649 0.0002649       0.002270
## p[8]    0.9345   0.02716 0.0002716       0.002190
## p[9]    0.9270   0.03109 0.0003109       0.002260
## sigma   0.2209   0.22010 0.0022010       0.012480
## tau   192.6153 394.00949 3.9400949      18.059368
## w       0.9301   0.03455 0.0003455       0.002301
## y       2.6927   0.50878 0.0050878       0.038462
## 
## 2. Quantiles for each variable:
## 
##          2.5%      25%     50%      75%     97.5%
## mu    1.98519  2.40252  2.6795   2.9214    3.5213
## p[1]  0.87541  0.91712  0.9370   0.9521    0.9740
## p[2]  0.84875  0.91163  0.9339   0.9497    0.9739
## p[3]  0.86589  0.91417  0.9343   0.9496    0.9723
## p[4]  0.86317  0.91383  0.9344   0.9496    0.9728
## p[5]  0.85602  0.91331  0.9345   0.9502    0.9734
## p[6]  0.86695  0.91580  0.9368   0.9522    0.9761
## p[7]  0.87368  0.91776  0.9372   0.9525    0.9751
## p[8]  0.87302  0.91814  0.9383   0.9540    0.9764
## p[9]  0.85399  0.91113  0.9328   0.9478    0.9715
## sigma 0.02749  0.07251  0.1432   0.2945    0.8244
## tau   1.47138 11.53178 48.7486 190.1751 1323.1462
## w     0.85492  0.91462  0.9355   0.9517    0.9754
## y     1.77375  2.37134  2.6751   2.9801    3.6819</code></pre>
</div>
